import torch.nn as nn
from torch_geometric.nn import Linear, GCNConv, GATv2Conv, GINEConv, GraphSAGE, SAGEConv, global_mean_pool
import torch.nn.functional as F
import torch

class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):
        super(GCN, self).__init__()
        self.dropout_rate = dropout_rate
        self.gcn1 = GCNConv(input_dim, hidden_dim)
        self.gcn2 = GCNConv(hidden_dim, hidden_dim)

        self.lin = Linear(hidden_dim, output_dim)
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, GCNConv) or isinstance(m, Linear):
                m.reset_parameters()

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.gcn1(x, edge_index)
        x = F.relu(x)

        x = self.gcn2(x, edge_index)
        x = F.relu(x)

        x = global_mean_pool(x, data.batch)

        x = F.dropout(x, self.dropout_rate)

        x = self.lin(x)

        return x.log_softmax(dim=-1)


class GAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, heads, dropout_rate):
        super(GAT, self).__init__()
        self.dropout_rate = dropout_rate

        self.conv1 = GATv2Conv(input_dim, hidden_dim, heads=heads)
        self.conv2 = GATv2Conv(hidden_dim * heads, hidden_dim, heads=heads)

        self.lin = nn.Linear(hidden_dim * heads, output_dim)
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, GCNConv) or isinstance(m, Linear):
                m.reset_parameters()

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = F.relu(x)

        x = self.conv2(x, edge_index)
        x = F.relu(x)

        x = global_mean_pool(x, data.batch)

        x = F.dropout(x, self.dropout_rate)

        x = self.lin(x)

        return x.log_softmax(dim=-1)


class GraphSAGE(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):
        super(GraphSAGE, self).__init__()
        self.dropout_rate = dropout_rate
        self.sage1 = SAGEConv(input_dim, hidden_dim)
        self.sage2 = SAGEConv(hidden_dim, hidden_dim)

        self.lin = Linear(hidden_dim, output_dim)
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, GCNConv) or isinstance(m, Linear):
                m.reset_parameters()

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.sage1(x, edge_index)
        x = F.relu(x)

        x = self.sage2(x, edge_index)
        x = F.relu(x)

        x = global_mean_pool(x, data.batch)

        x = F.dropout(x, self.dropout_rate)

        x = self.lin(x)

        return x.log_softmax(dim=-1)


class LSTM(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, num_layers=1):
        super(LSTM, self).__init__()
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)
        # x -> (batch_size, seq, input_size)
        # self.fc = nn.Linear(hidden_dim, output_dim)
        # self.fc = nn.Linear(hidden_dim*sequence_length, output_dim)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)  # hidden state

        out, _ = self.lstm(x, (h0, c0))
        out = out[:, -1, :]
        # out: batch_size, seq_length, hidden_size
        # out = self.fc(out)

        return out


class GRU(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, num_layers=1):
        super(GRU, self).__init__()
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        # x -> (batch_size, seq, input_size)
        # self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)

        out, _ = self.gru(x, h0)
        out = out[:, -1, :]
        # out: batch_size, seq_length, hidden_size
        # out = self.fc(out)

        return out
